[{"id":"24242b49aa34da839d61a817de58ead5","title":"OpenStack概览","content":"OpenStack概览OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。\nOpenStack不是虚拟化1、  OpenStack的架构定位与技术范畴\n\nOpenstack只是系统的控制面。\nOpenStack不包括系统的数据面组件，如Hypervisor、存储和网络设备等\n\n2、OpenStack和虚拟化有着关键的区别：\n虚拟化是OpenStack底层的技术实现手段之一，但非核心关注点\nOpenStack不是云计算1、OpenStack只是构建云计算的关键组件：\n** 内核、骨干、框架、总线 **\n2、为了构建云计算，我们还需要很多东西：\nOpenStack的设计思想1、开放\n\n开源，并尽可能的重用已有开源项目\n不要“重复发明轮子”\n\n2、灵活\n\n不使用任何不可代替的商用或私有的组件\n大量使用插件方式进行架构设计与实现\n\n3、可扩展\n\n由多个相互独立的项目组成\n每个项目包含多个独立服务组件\n无中心架构\n无状态架构\n\nOpenStack历史版本OpenStack每年更新两个大版本，一般在4月和10月中旬发布，版本命名从字母A-Z。\nOpenStack架构\nOpenStack生产环境部署架构示例\nOpenStack核心服务简介\n\n\n\n\n\n\n\n\nOpenStack服务间交互示例\n1、 OpenStack创建VM，服务间交互示例\n\n","slug":"os-intro","date":"2022-07-16T18:38:34.000Z","categories_index":"OpenStack,Linux","tags_index":"Linux,OpenStack","author_index":"网工的狗"},{"id":"85ab71e52051db561aa2a8f46a8893b4","title":"Docker介绍","content":"什么是Docker？Docker是世界领先的软件容器平台，所以想要搞懂Docker的概念我们必须先从容器开始说起。容器是完全使用沙箱机制，相互之间不会有任何接口,容器性能开销极低。\nDocker是虚拟化技术的一种，利用Linux核心中的资源分脱机制，例如 cgroups，以及 Linux 核心名字空间（name space），来创建独立的软件容器（containers），属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docker 在容器的基础上进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护，使得其比虚拟机技术更为轻便、快捷。Docker 可以在单一Linux实体下运作，避免因为创建一个虚拟机而造成的额外负担。\nDocker与虚拟化“虚拟化”是一项技术，也是一种资源解决方案。虚拟化技术是将物理资源转变为逻辑上可以管理的资源，以打破物理结构之间的壁垒，使计算元件运行在虚拟的基础上，而不是真实的物理资源上。通过虚拟化技术，可以将物理资源转变为逻辑资源（虚拟机），应用程序服务运行在虚拟资源上，而不是真实的物理机上。\n虚拟化技术的落地，底层就必须有物理机支撑。单纯的物理机是不能直接虚拟化的，都需要虚拟化软件来实现。目前主流的虚拟化软件有：KVM，XEN，ESXI，HP-V，Docker，Virtual BOX等。\n虚拟化技术的应用场景企业需求：部署一百套Nginx WEB服务，要求对外端口为80，要求独立服务器部署\n1、传统方案：采购一百台低配硬件物理机，每台物理机部署一套Nginx WEB服务\n2、虚拟化方案：采购10台高配硬件物理机，每台物理机虚拟10台虚拟机，每台虚拟机独立部署一套Nginx WEB服务\n第二种方案从成本、部署难度、维护等方面，第二种都是比第一种要好。\n虚拟化的意义在于：对于硬件设备资源的最大化利用，降低企业各种费用成本，简化后期资源部署和维护，动态满足企业需求，基于虚拟化资源来代替待淘汰的物理资源\nDocker虚拟化相比传统虚拟化的优点简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。\n传统虚拟化技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。\n容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。\n\n\n容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动。\n\n虚拟机（VM）是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个VM在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此占用大量空间。而且VM启动也十分缓慢。\n\n\n两者有不同的使用场景，虚拟机更擅长于彻底隔离整个运行环境，而Docker通常用于隔离不同的应用。\n容器技术和我们的宿主机共享硬件资源及操作系统，可以实现资源的动态分配。容器包含应用和其所有的依赖包，但是与其他容器共享内核。容器在宿主机操作系统中，在用户空间以分离的进程运行。容器内没有自己的内核，也没有进行硬件虚拟。\n具体来说与虚拟机技术对比，Docker 容器存在以下几个特点：\n1、更快的启动速度：因为 Docker 直接运行于宿主内核，无需启动完整的操作系统，因此启动速度属于秒级别，而虚拟机通常需要几分钟去启动。\n2、更高效的资源利用率：由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。\n3、更高的系统支持量：Docker 的架构可以共用一个内核与共享应用程序库，所占内存极小。同样的硬件环境，Docker 运行的镜像数远多于虚拟机数量，对系统的利用率非常高。\n4、持续交付与部署：对开发和运维人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至进行自动部署。\n5、更轻松的迁移：由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。\n6、更轻松的维护与扩展：Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。\n7、更弱的隔离性：Docker 属于进程之间的隔离，虚拟机可实现系统级别隔离。\n8、更弱的安全性：Docker 的租户 root 和宿主机 root 等同，一旦容器内的用户从普通用户权限提升为 root 权限，它就直接具备了宿主机的 root 权限，进而可进行无限制的操作。虚拟机租户 root 权限和宿主机的 root 虚拟机权限是分离的，并且利用硬件隔离技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离，这使得容器容易受到攻击。\nDocker基本概念Docker包括三个基本概念：\n\n镜像（Image）\n容器（Container）\n仓库（Repository）\n\n镜像（Image）——一个特殊的文件系统\n操作系统分为内核和用户空间。对于Linux而言，内核启动后，会挂载root文件系统为其提供用户空间支持。而Docker镜像（Image），就相当于是一个root文件系统。\nDocker镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。\nDocker设计时，就充分利用Union FS的技术，将其设计为分层存储的架构。 镜像实际是由多层文件系统联合组成。\n镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。\n分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。\n容器（Container）——镜像运行时的实体\n镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。\n容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。前面讲过镜像使用的是分层存储，容器也是如此。\n容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n按照Docker最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新run，数据却不会丢失。\n仓库（Repository）——集中存放镜像文件的地方\n镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。\n一个Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。\n通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以latest作为默认标签。\nDocker三剑客docker-compose：Docker 镜像在创建之后，往往需要自己手动 pull 来获取镜像，然后执行 run 命令来运行。当服务需要用到多种容器，容器之间又产生了各种依赖和连接的时候，部署一个服务的手动操作是令人感到十分厌烦的。\ndcoker-compose 技术，就是通过一个 .yml 配置文件，将所有的容器的部署方法、文件映射、容器连接等等一系列的配置写在一个配置文件里，最后只需要执行 docker-compose up 命令就会像执行脚本一样的去一个个安装容器并自动部署他们，极大的便利了复杂服务的部署。\ndocker-machine：Docker 技术是基于 Linux 内核的 cgroup 技术实现的，那么问题来了，在非 Linux 平台上是否就不能使用 docker 技术了呢？答案是可以的，不过显然需要借助虚拟机去模拟出 Linux 环境来。\ndocker-machine 就是 docker 公司官方提出的，用于在各种平台上快速创建具有 docker 服务的虚拟机的技术，甚至可以通过指定 driver 来定制虚拟机的实现原理（一般是 virtualbox）。\ndocker-swarm：swarm 是基于 docker 平台实现的集群技术， 他可以通过几条简单的指令快速的创建一个 docker 集群，接着在集群的共享网络上部署应用，最终实现分布式的服务。\nDocker的基本组成1、ocker Client 客户端\n2、Docker Daemon 守护进程\n3、Docker Image 镜像\n4、Docker Container 容器\n5、Docker Registry 仓库\n\ndocker镜像是一个层叠的只读文件系统(联合文件系统)：\n最低端是一个引导文件系统 bootfs\n第二层是rootfs，位于引导文件系统之上，可以是一种或多种操作系统(redhat&#x2F;ubuntu等)，在docker中，rootfs永远是只读方式，并且利用联合加载技术(union mount)，加载更多的只读文件系统\n\nDocker容器docker容器通过docker镜像启动，是Docker镜像运行的实体，是活动的。\n当一个容器启动时，会再docker镜像的最顶层加载一个读写文件系统，docker中运行的程序就是在该层进行运行的，第一次启动是，初始化的读写层是空的，所有的写操作都应用在该层(从只读的底层复制到读写层[写时复制技术])\nDocker镜像和容器的关系，跟程序和进程的关系类似\nDocker仓库存放Docker镜像的地方，一般分为公共仓库和私有仓库，docker公司提供了一个自己的仓库”Docker Hub”\nDocker的相关技术Docker依赖Linux的内核特性有：Cgroup(控制组)和Namespace(命名空间)\nNamespace命名空间是一种封装的概念，在操作系统层面上，提供了系统资源的隔离，系统资源包括{进程、文件系统、网络等}。Linux实现命名空间的目的：为了实现轻量级虚拟化服务，在不同命名空间下的进程，彼此毫无关系。\ndocker使用了五种命名空间：\nPID 进程隔离\n\nNET 管理网络接口\n\nIPC 管理进程间通信\n\nMNT 管理挂载点，文件系统间的隔离\n\nUTS 管理内核和版本标示的隔离\n\n\nCgroupCgroup对隔离的资源进行管理，是一种用来限制、记录，隔离进程组资源的机制。就是为了容器技术而生的。\nCgroup对资源的管理方式资源限制 如：对内存的分配上限\n优先级设定 如：某个进程优先使用cpu时间片\n资源计量 计算进程组使用了多少系统资源，尤其时在记费系统中\n资源控制 将进程组挂起或恢复\nDocker容器的能力文件系统隔离：每个容器都有自己的root文件系统\n进程隔离：每个容器都运行在自己的进程环境中，互相不影响\n网络隔离：容器间的虚拟网络接口和IP地址都是分开的\n资源隔离和分组：使用Cgroups将CPU和内存之类的物理资源独立的分配给每个Docker容器\n","slug":"docker-introduce","date":"2022-07-06T01:12:31.000Z","categories_index":"Docker,Linux","tags_index":"Linux,Docker","author_index":"网工的狗"},{"id":"73d192d4467e8265f5454c782a51beb8","title":"Ceph架构简介及使用场景介绍","content":"Ceph简介Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。\nCeph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。\nCeph特点\n高性能\n1、摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。\n2、考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。\n3、能够支持上千个存储节点的规模，支持TB到PB级的数据。\n\n高可用性\n1、副本数可以灵活控制。\n2、支持故障域分隔，数据强一致性。\n3、多种故障场景自动进行修复自愈。\n4、没有单点故障，自动管理。\n\n高可扩展性\n1、去中心化。\n2、扩展灵活。\n3、随着节点增加而线性增长。\n\n特性丰富\n1、支持三种存储接口：块存储、文件存储、对象存储。\n2、支持自定义接口，支持多种语言驱动。\n\n\nCeph架构支持三种接口：\n\nObject：有原生的API，而且也兼容Swift和S3的API。\nBlock：支持精简配置、快照、克隆。\nFile：Posix接口，支持快照。\n\n\nCeph核心组件及概念介绍\nMonitor\n\n一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。\n\nOSD\n\nOSD全称Object Storage Device，也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。\n\nMDS\n\nMDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。\n\nObject\n\nCeph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。\n\nPG\n\nPG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。\n\nRADOS\n\nRADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。\n\nLibradio\n\nLibrados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。\n\nCRUSH\n\nCRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。\n\nRBD\n\nRBD全称RADOS block device，是Ceph对外提供的块设备服务。\n\nRGW\n\nRGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。\n\nCephFS\n\nCephFS全称Ceph File System，是Ceph对外提供的文件系统服务。\n三种存储类型块存储典型设备： 磁盘阵列，硬盘\n主要是将裸磁盘空间映射给主机使用的。\n优点：\n\n通过Raid与LVM等手段，对数据提供了保护。\n多块廉价的硬盘组合起来，提高容量。\n多块磁盘组合出来的逻辑盘，提升读写效率。\n\n缺点：\n\n采用SAN架构组网时，光纤交换机，造价成本高。\n主机之间无法共享数据。\n\n使用场景：\n\ndocker容器、虚拟机磁盘存储分配。\n日志存储。\n文件存储。\n…\n\n文件存储典型设备： FTP、NFS服务器为了克服块存储文件无法共享的问题，所以有了文件存储。在服务器上架设FTP与NFS服务，就是文件存储。\n优点：\n\n造价低，随便一台机器就可以了。\n方便文件共享。\n\n缺点：\n\n读写速率低。\n传输速率慢。\n\n使用场景：\n\n日志存储。\n有目录结构的文件存储。\n…\n\n对象存储典型设备： 内置大容量硬盘的分布式服务器(swift, s3)\n多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。\n优点：\n\n具备块存储的读写高速。\n具备文件存储的共享等特性。\n\n使用场景： (适合更新变动较少的数据)\n\n图片存储。\n视频存储。\n…\n\nIO流程及数据分布\n正常IO流程图\n步骤：\n\nclient 创建cluster handler。\nclient 读取配置文件。\nclient 连接上monitor，获取集群map信息。\nclient 读写io 根据crshmap 算法请求对应的主osd数据节点。\n主osd数据节点同时写入另外两个副本节点数据。\n等待主节点以及另外两个副本节点写完数据状态。\n主节点及副本节点写入状态都成功后，返回给client，io写入完成。\n\n新主IO流程图说明：\n如果新加入的OSD1取代了原有的 OSD4成为 Primary OSD, 由于 OSD1 上未创建 PG , 不存在数据，那么 PG 上的 I&#x2F;O 无法进行，怎样工作的呢？\n\n步骤：\n\nclient连接monitor获取集群map信息。\n同时新主osd1由于没有pg数据会主动上报monitor告知让osd2临时接替为主。\n临时主osd2会把数据全量同步给新主osd1。\nclient IO读写直接连接临时主osd2进行读写。\nosd2收到读写io，同时写入另外两副本节点。\n等待osd2以及另外两副本写入成功。\nosd2三份数据都写入成功返回给client, 此时client io读写完毕。\n如果osd1数据同步完毕，临时主osd2会交出主角色。\nosd1成为主节点，osd2变成副本。\n\nCeph IO算法流程\n\nFile用户需要读写的文件。File-&gt;Object映射：a. ino (File的元数据，File的唯一id)。b. ono(File切分产生的某个object的序号，默认以4M切分一个块大小)。c. oid(object id: ino + ono)。\n\nObject是RADOS需要的对象。Ceph指定一个静态hash函数计算oid的值，将oid映射成一个近似均匀分布的伪随机值，然后和mask按位相与，得到pgid。Object-&gt;PG映射：a. hash(oid) &amp; mask-&gt; pgid 。b. mask &#x3D; PG总数m(m为2的整数幂)-1 。\n\nPG(Placement Group),用途是对object的存储进行组织和位置映射, (类似于redis cluster里面的slot的概念) 一个PG里面会有很多object。采用CRUSH算法，将pgid代入其中，然后得到一组OSD。PG-&gt;OSD映射：a. CRUSH(pgid)-&gt;(osd1,osd2,osd3) 。\n\n\nCeph IO伪代码流程locator &#x3D; object_name\nobj_hash &#x3D;  hash(locator)\npg &#x3D; obj_hash % num_pg\nosds_for_pg &#x3D; crush(pg)    # returns a list of osds\nprimary &#x3D; osds_for_pg[0]\nreplicas &#x3D; osds_for_pg[1:]\n\nCeph RBD IO流程\n步骤：\n\n客户端创建一个pool，需要为这个pool指定pg的数量。\n创建pool&#x2F;image rbd设备进行挂载。\n用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。\n将每个object通过pg进行副本位置的分配。\npg根据cursh算法会寻找3个osd，把这个object分别保存在这三个osd上。\nosd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。\nobject的存储就变成了存储一个文rbd0.object1.file。\n\nCeph RBD IO框架图\n客户端写数据osd过程：\n\n采用的是librbd的形式，使用librbd创建一个块设备，向这个块设备中写入数据。\n在客户端本地同过调用librados接口，然后经过pool，rbd，object、pg进行层层映射,在PG这一层中，可以知道数据保存在哪3个OSD上，这3个OSD分为主从的关系。\n客户端与primay OSD建立SOCKET 通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点。\n\nCeph Pool和PG分布情况\n说明：\n\npool是ceph存储数据时的逻辑分区，它起到namespace的作用。\n每个pool包含一定数量(可配置)的PG。\nPG里的对象被映射到不同的OSD上。\npool是分布到整个集群的。\npool可以做故障隔离域，根据不同的用户场景不一进行隔离。\n\nCeph 数据扩容PG分布场景数据迁移流程：\n\n现状3个OSD, 4个PG\n扩容到4个OSD, 4个PG\n\n现状：\n扩容后：\n说明\n每个OSD上分布很多PG, 并且每个PG会自动散落在不同的OSD上。如果扩容那么相应的PG会进行迁移到新的OSD上，保证PG数量的均衡。\nCeph心跳机制心跳介绍心跳是用于节点间检测对方是否故障的，以便及时发现故障节点进入相应的故障处理流程。\n问题：\n\n故障检测时间和心跳报文带来的负载之间做权衡。\n心跳频率太高则过多的心跳报文会影响系统性能。\n心跳频率过低则会延长发现故障节点的时间，从而影响系统的可用性。\n\n故障检测策略应该能够做到：\n\n及时：节点发生异常如宕机或网络中断时，集群可以在可接受的时间范围内感知。\n适当的压力：包括对节点的压力，和对网络的压力。\n容忍网络抖动：网络偶尔延迟。\n扩散机制：节点存活状态改变导致的元信息变化需要通过某种机制扩散到整个集群。\n\nCeph 心跳检测\nOSD节点会监听public、cluster、front和back四个端口\n\npublic端口：监听来自Monitor和Client的连接。\ncluster端口：监听来自OSD Peer的连接。\nfront端口：供客户端连接集群使用的网卡, 这里临时给集群内部之间进行心跳。\nback端口：供客集群内部使用的网卡。集群内部之间进行心跳。\nhbclient：发送ping心跳的messenger。\n\nCeph OSD之间相互心跳检测步骤：\n\n同一个PG内OSD互相心跳，他们互相发送PING&#x2F;PONG信息。\n每隔6s检测一次(实际会在这个基础上加一个随机时间来避免峰值)。\n20s没有检测到心跳回复，加入failure队列。\n\nCeph OSD与Mon心跳检测\nOSD报告给Monitor：\n\nOSD有事件发生时（比如故障、PG变更）。\n自身启动5秒内。\nOSD周期性的上报给Monito\nOSD检查failure_queue中的伙伴OSD失败信息。\n向Monitor发送失效报告，并将失败信息加入failure_pending队列，然后将其从failure_queue移除。\n收到来自failure_queue或者failure_pending中的OSD的心跳时，将其从两个队列中移除，并告知Monitor取消之前的失效报告。\n当发生与Monitor网络重连时，会将failure_pending中的错误报告加回到failure_queue中，并再次发送给Monitor。\n\n\nMonitor统计下线OSD\nMonitor收集来自OSD的伙伴失效报告。\n当错误报告指向的OSD失效超过一定阈值，且有足够多的OSD报告其失效时，将该OSD下线。\n\n\n\nCeph心跳检测总结Ceph通过伙伴OSD汇报失效节点和Monitor统计来自OSD的心跳两种方式判定OSD节点失效。\n\n及时：伙伴OSD可以在秒级发现节点失效并汇报Monitor，并在几分钟内由Monitor将失效OSD下线。\n适当的压力：由于有伙伴OSD汇报机制，Monitor与OSD之间的心跳统计更像是一种保险措施，因此OSD向Monitor发送心跳的间隔可以长达600秒，Monitor的检测阈值也可以长达900秒。Ceph实际上是将故障检测过程中中心节点的压力分散到所有的OSD上，以此提高中心节点Monitor的可靠性，进而提高整个集群的可扩展性。\n容忍网络抖动：Monitor收到OSD对其伙伴OSD的汇报后，并没有马上将目标OSD下线，而是周期性的等待几个条件：\n目标OSD的失效时间大于通过固定量osd_heartbeat_grace和历史网络条件动态确定的阈值。\n来自不同主机的汇报达到mon_osd_min_down_reporters。\n满足前两个条件前失效汇报没有被源OSD取消。\n\n\n扩散：*作为中心节点的Monitor并没有在更新OSDMap后尝试广播通知所有的OSD和Client，而是惰性的等待OSD和Client来获取。以此来减少Monitor压力并简化交互逻辑。\n\nCeph通信框架Ceph通信框架种类介绍网络通信框架三种不同的实现方式：\n\nSimple线程模式特点：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送。缺点：大量的链接会产生大量的线程，会消耗CPU资源，影响性能。\nAsync事件的I&#x2F;O多路复用模式特点：这种是目前网络通信中广泛采用的方式。k版默认已经使用Asnyc了。\nXIO方式使用了开源的网络通信库accelio来实现特点：这种方式需要依赖第三方的库accelio稳定性，目前处于试验阶段。\n\nCeph通信框架设计模式设计模式(Subscribe&#x2F;Publish)：\n订阅发布模式又名观察者模式，它意图是“定义对象间的一种一对多的依赖关系，\n当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新”。\nCeph通信框架流程图\n步骤：\n\nAccepter监听peer的请求, 调用 SimpleMessenger::add_accept_pipe() 创建新的 Pipe 到 SimpleMessenger::pipes 来处理该请求。\nPipe用于消息的读取和发送。该类主要有两个组件，Pipe::Reader，Pipe::Writer用来处理消息读取和发送。\nMessenger作为消息的发布者, 各个 Dispatcher 子类作为消息的订阅者, Messenger 收到消息之后，  通过 Pipe 读取消息，然后转给 Dispatcher 处理。\nDispatcher是订阅者的基类，具体的订阅后端继承该类,初始化的时候通过 Messenger::add_dispatcher_tail&#x2F;head 注册到 Messenger::dispatchers. 收到消息后，通知该类处理。\nDispatchQueue该类用来缓存收到的消息, 然后唤醒 DispatchQueue::dispatch_thread 线程找到后端的 Dispatch 处理消息。\n\n\nCeph通信框架类图\nCeph通信数据格式通信协议格式需要双方约定数据格式。\n消息的内容主要分为三部分：\n\nheader              &#x2F;&#x2F;消息头，类型消息的信封\nuser data          &#x2F;&#x2F;需要发送的实际数据\npayload     &#x2F;&#x2F;操作保存元数据\nmiddle      &#x2F;&#x2F;预留字段\ndata          &#x2F;&#x2F;读写数据\n\n\nfooter             &#x2F;&#x2F;消息的结束标记class Message : public RefCountedObject &#123;\nprotected:\n  ceph_msg_header  header;      &#x2F;&#x2F; 消息头\n  ceph_msg_footer  footer;\t\t&#x2F;&#x2F; 消息尾\n  bufferlist       payload;  &#x2F;&#x2F; &quot;front&quot; unaligned blob\n  bufferlist       middle;   &#x2F;&#x2F; &quot;middle&quot; unaligned blob\n  bufferlist       data;     &#x2F;&#x2F; data payload (page-alignment will be preserved where possible)\n\n  &#x2F;* recv_stamp is set when the Messenger starts reading the\n   * Message off the wire *&#x2F;\n  utime_t recv_stamp;\t\t&#x2F;&#x2F;开始接收数据的时间戳\n  &#x2F;* dispatch_stamp is set when the Messenger starts calling dispatch() on\n   * its endpoints *&#x2F;\n  utime_t dispatch_stamp;\t&#x2F;&#x2F;dispatch 的时间戳\n  &#x2F;* throttle_stamp is the point at which we got throttle *&#x2F;\n  utime_t throttle_stamp;\t&#x2F;&#x2F;获取throttle 的slot的时间戳\n  &#x2F;* time at which message was fully read *&#x2F;\n  utime_t recv_complete_stamp;\t&#x2F;&#x2F;接收完成的时间戳\n\n  ConnectionRef connection;\t\t&#x2F;&#x2F;网络连接\n\n  uint32_t magic &#x3D; 0;\t\t\t&#x2F;&#x2F;消息的魔术字\n\n  bi::list_member_hook&lt;&gt; dispatch_q;\t&#x2F;&#x2F;boost::intrusive 成员字段\n&#125;;\n\nstruct ceph_msg_header &#123;\n\t__le64 seq;       &#x2F;&#x2F; 当前session内 消息的唯一 序号\n\t__le64 tid;       &#x2F;&#x2F; 消息的全局唯一的 id\n\t__le16 type;      &#x2F;&#x2F; 消息类型\n\t__le16 priority;  &#x2F;&#x2F; 优先级\n\t__le16 version;   &#x2F;&#x2F; 版本号\n\n\t__le32 front_len; &#x2F;&#x2F; payload 的长度\n\t__le32 middle_len;&#x2F;&#x2F; middle 的长度\n\t__le32 data_len;  &#x2F;&#x2F; data 的 长度\n\t__le16 data_off;  &#x2F;&#x2F; 对象的数据偏移量\n\n\n\tstruct ceph_entity_name src; &#x2F;&#x2F;消息源\n\n\t&#x2F;* oldest code we think can decode this.  unknown if zero. *&#x2F;\n\t__le16 compat_version;\n\t__le16 reserved;\n\t__le32 crc;       &#x2F;* header crc32c *&#x2F;\n&#125; __attribute__ ((packed));\n\nstruct ceph_msg_footer &#123;\n\t__le32 front_crc, middle_crc, data_crc; &#x2F;&#x2F;crc校验码\n\t__le64  sig; &#x2F;&#x2F;消息的64位signature\n\t__u8 flags; &#x2F;&#x2F;结束标志\n&#125; __attribute__ ((packed));\n\nCeph CRUSH算法数据分布算法挑战\n数据分布和负载均衡：a. 数据分布均衡，使数据能均匀的分布到各个节点上。b. 负载均衡，使数据访问读写操作的负载在各个节点和磁盘的负载均衡。\n灵活应对集群伸缩a. 系统可以方便的增加或者删除节点设备，并且对节点失效进行处理。b. 增加或者删除节点设备后，能自动实现数据的均衡，并且尽可能少的迁移数据。\n支持大规模集群a. 要求数据分布算法维护的元数据相对较小，并且计算量不能太大。随着集群规模的增 加，数据分布算法开销相对比较小。\n\nCeph CRUSH算法说明\nCRUSH算法的全称为：Controlled Scalable Decentralized Placement of Replicated Data，可控的、可扩展的、分布式的副本数据放置算法。\npg到OSD的映射的过程算法叫做CRUSH 算法。(一个Object需要保存三个副本，也就是需要保存在三个osd上)。\nCRUSH算法是一个伪随机的过程，他可以从所有的OSD中，随机性选择一个OSD集合，但是同一个PG每次随机选择的结果是不变的，也就是映射的OSD集合是固定的。\n\nCeph CRUSH算法原理CRUSH算法因子：\n\n层次化的Cluster Map反映了存储系统层级的物理拓扑结构。定义了OSD集群具有层级关系的 静态拓扑结构。OSD层级使得 CRUSH算法在选择OSD时实现了机架感知能力，也就是通过规则定义， 使得副本可以分布在不同的机 架、不同的机房中、提供数据的安全性 。\nPlacement Rules决定了一个PG的对象副本如何选择的规则，通过这些可以自己设定规则，用户可以自定义设置副本在集群中的分布。\n\n层级化的Cluster Map\nCRUSH Map是一个树形结构，OSDMap更多记录的是OSDMap的属性(epoch&#x2F;fsid&#x2F;pool信息以及osd的ip等等)。\n叶子节点是device（也就是osd），其他的节点称为bucket节点，这些bucket都是虚构的节点，可以根据物理结构进行抽象，当然树形结构只有一个最终的根节点称之为root节点，中间虚拟的bucket节点可以是数据中心抽象、机房抽象、机架抽象、主机抽象等。\n数据分布策略Placement Rules数据分布策略Placement Rules主要有特点：\na. 从CRUSH Map中的哪个节点开始查找b. 使用那个节点作为故障隔离域c. 定位副本的搜索模式（广度优先 or 深度优先）\nrule replicated_ruleset  #规则集的命名，创建pool时可以指定rule集\n&#123;\n    ruleset 0                #rules集的编号，顺序编即可   \n    type replicated          #定义pool类型为replicated(还有erasure模式)   \n    min_size 1                #pool中最小指定的副本数量不能小1\n    max_size 10               #pool中最大指定的副本数量不能大于10       \n    step take default         #查找bucket入口点，一般是root类型的bucket    \n    step chooseleaf  firstn  0  type  host #选择一个host,并递归选择叶子节点osd     \n    step emit        #结束\n&#125;\n\nBucket随机算法类型\n\n一般的buckets：适合所有子节点权重相同，而且很少添加删除item。\nlist buckets：适用于集群扩展类型。增加item，产生最优的数据移动，查找item，时间复杂度O(n)。\ntree buckets：查找负责度是O (log n), 添加删除叶子节点时，其他节点node_id不变。\nstraw buckets：允许所有项通过类似抽签的方式来与其他项公平“竞争”。定位副本时，bucket中的每一项都对应一个随机长度的straw，且拥有最长长度的straw会获得胜利（被选中），添加或者重新计算，子树之间的数据移动提供最优的解决方案。\n\nCRUSH算法案例说明：\n集群中有部分sas和ssd磁盘，现在有个业务线性能及可用性优先级高于其他业务线，能否让这个高优业务线的数据都存放在ssd磁盘上。\n普通用户：\n\n高优用户：\n\n配置规则：\n\n定制化Ceph RBD QOSQOS介绍QoS （Quality of Service，服务质量）起源于网络技术，它用来解决网络延迟和阻塞等问题，能够为指定的网络通信提供更好的服务能力。\n问题：\n我们总的Ceph集群的iIO能力是有限的，比如带宽，IOPS。如何避免用户争取资源，如果保证集群所有用户资源的高可用性，以及如何保证高优用户资源的可用性。所以我们需要把有限的IO能力合理分配。\nCeph IO操作类型\nClientOp：来自客户端的读写I&#x2F;O请求。\nSubOp：osd之间的I&#x2F;O请求。主要包括由客户端I&#x2F;O产生的副本间数据读写请求，以及由数据同步、数据扫描、负载均衡等引起的I&#x2F;O请求。\nSnapTrim：快照数据删除。从客户端发送快照删除命令后，删除相关元数据便直接返回，之后由后台线程删除真实的快照数据。通过控制snaptrim的速率间接控制删除速率。\nScrub：用于发现对象的静默数据错误，扫描元数据的Scrub和对象整体扫描的deep Scrub。\nRecovery：数据恢复和迁移。集群扩&#x2F;缩容、osd失效&#x2F;从新加入等过程。\n\nCeph 官方QOS原理\nmClock是一种基于时间标签的I&#x2F;O调度算法，最先被Vmware提出来的用于集中式管理的存储系统。(目前官方QOS模块属于半成品)。\n基本思想：\n\nreservation 预留，表示客户端获得的最低I&#x2F;O资源。\nweight 权重，表示客户端所占共享I&#x2F;O资源的比重。\nlimit 上限，表示客户端可获得的最高I&#x2F;O资源。\n\n定制化QOS原理令牌桶算法介绍\n基于令牌桶算法(TokenBucket)实现了一套简单有效的qos功能，满足了云平台用户的核心需求。\n基本思想：\n\n按特定的速率向令牌桶投放令牌。\n根据预设的匹配规则先对报文进行分类，不符合匹配规则的报文不需要经过令牌桶的处理，直接发送。\n符合匹配规则的报文，则需要令牌桶进行处理。当桶中有足够的令牌则报文可以被继续发送下去，同时令牌桶中的令牌量按报文的长度做相应的减少。\n当令牌桶中的令牌不足时，报文将不能被发送，只有等到桶中生成了新的令牌，报文才可以发送。这就可以限制报文的流量只能是小于等于令牌生成的速度，达到限制流量的目的。\n\nRBD令牌桶算法流程\n步骤：\n\n用户发起请求异步IO到达Image中。\n请求到达ImageRequestWQ队列中。\n在ImageRequestWQ出队列的时候加入令牌桶算法TokenBucket。\n通过令牌桶算法进行限速，然后发送给ImageRequest进行处理。\n\nRBD令牌桶算法框架图现有框架图：\n\n令牌图算法框架图：\n\n","slug":"ceph-introduce","date":"2022-07-05T15:22:49.000Z","categories_index":"","tags_index":"Linux,Ceph","author_index":"网工的狗"},{"id":"71ccfa0a8479931c66dcff584c51ed03","title":"计算机组成原理笔记","content":"这门课完全可以用一个词来概括，就是“抽象”。在我看来这也是整个计算机设计中所蕴含的的灵魂。\n\n\n\nCPU性能响应时间：指的就是，我们执行一个程序，到底需要花多少时间。花的时间越少，自然性能就越好。\n吞吐率：在一定的时间范围内，到底能处理多少事情。这里的“事情”，在计算机里就是处理的数据或者执行的程序指令。\n我们一般把性能，定义成响应时间的倒数，也就是：\n性能 &#x3D; 1&#x2F;响应时间\n\n程序运行的时间程序运行的时间&#x3D;程序运行结束的时间-程序开始运行的时间\n\n但是，计算机可能同时运行着好多个程序，CPU实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能CPU切换去运行别的程序了。所以这个时间并不准。\n我们使用time命令统计运行时间：\n$ time seq\n  1000000 | wc -l \n1000000 \nreal 0m0.101s \nuser 0m0.031s \nsys  0m0.016s\n\n其中real就是Wall Clock Time，而程序实际花费的CPU执行时间，就是user time加上sys time。\n我们下面对程序的CPU执行时间进行拆解：\n程序的CPU执行时间&#x3D;CPU时钟周期数×时钟周期时间\n\n时钟周期时间：如果一台电脑的主频是2.8GHz，那么可以简单认为，CPU在1秒时间内，可以执行的简单指令的数量是2.8G条。在这个2.8GHz的CPU上，这个时钟周期时间，就是1&#x2F;2.8G。\n对于上面的公式：CPU时钟周期数还可以拆解成指令数×每条指令的平均时钟周期数Cycles Per Instruction，简称CPI）。\n程序的CPU执行时间&#x3D;指令数×CPI×Clock Cycle Time\n\n并行优化由于通过提升CPU频率已经达到瓶颈，所以开始推出多核CPU，通过提升“吞吐率”而不是“响应时间”，来达到目的。\n但是，并不是所有问题，都可以通过并行提高性能来解决。如果想要使用这种思想，需要满足这样几个条件。\n1、需要进行的计算，本身可以分解成几个可以并行的任务。2、需要能够分解好问题，并确保几个人的结果能够汇总到一起。3、在“汇总”这个阶段，是没有办法并行进行的，还是得顺序执行，一步一步来。\n所以并行计算涉及到了一个阿姆达尔定律（Amdahl’s Law）。\n对于一个程序进行优化之后，处理器并行运算之后效率提升的情况。具体可以用这样一个公式来表示：\n优化后的执行时间 &#x3D; 受优化影响的执行时间&#x2F;加速倍数+不受影响的执行时间\n比如做一段数据的计算， 本来如果整个计算单核完成需要120ns，但是我们可以将这个任务拆分成4个，最后再汇总加起来。\n如果每个任务单独计算需要25ns，加起来汇总需要20ns，那么4个任务并行计算需要100&#x2F;4+20&#x3D;25ns。\n即使我们增加更多的并行度来提供加速倍数，比如有100个CPU，整个时间也需要100&#x2F;100+20&#x3D;21ns。\n\n从编译到汇编，代码怎么变成机器码？如下C语言程序例子：\n&#x2F;&#x2F; test.cint main()&#123;\n  int a &#x3D; 1;\n  int b &#x3D; 2;\n  a &#x3D; a + b;\n&#125;\n我们给两个变量 a、b分别赋值1、2，然后再将a、b两个变量中的值加在一起，重新赋值给了a整个变量。\n要让这段程序在一个Linux操作系统上跑起来，我们需要把整个程序翻译成一个汇编语言（ASM，Assembly Language）的程序，这个过程我们一般叫编译（Compile）成汇编代码。\n针对汇编代码，我们可以再用汇编器（Assembler）翻译成机器码（Machine Code）。这些机器码由“0”和“1”组成的机器语言表示。这一条条机器码，就是一条条的计算机指令。这样一串串的16进制数字，就是我们CPU能够真正认识的计算机指令。\n\n汇编代码其实就是“给程序员看的机器码”，也正因为这样，机器码和汇编代码是一一对应的。我们人类很容易记住add、mov这些用英文表示的指令，而8b 45 f8这样的指令，由于很难一下子看明白是在干什么，所以会非常难以记忆。所以我们需要汇编代码。\n指令是如何被执行的一个CPU里面会有很多种不同功能的寄存器。我这里给你介绍三种比较特殊的。\n1、PC寄存器（Program Counter Register），也叫指令地址寄存器（Instruction Address Register）。它就是用来存放下一条需要执行的计算机指令的内存地址。\n2、指令寄存器（Instruction Register），用来存放当前正在执行的指令。\n3、条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放CPU进行算术或者逻辑计算的结果。\n\n实际上，一个程序执行的时候，CPU会根据PC寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。\n程序的执行和跳转现在就来看一个包含if…else的简单程序。\n&#x2F;&#x2F; test.c\n#include &lt;time.h&gt;\n#include &lt;stdlib.h&gt; \nint main() &#123; \n    srand(time(NULL)); \n    int r &#x3D; rand() % 2; \n    int a &#x3D; 10; \n    if (r &#x3D;&#x3D; 0) &#123; \n        a &#x3D; 1; \n        &#125; else &#123; \n        a &#x3D; 2; \n&#125;\n\n把这个程序编译成汇编代码。\nif (r &#x3D;&#x3D; 0)\n3b: 83 7d fc 00     cmp DWORD PTR [rbp-0x4],0x0 \n3f: 75 09           jne 4a &lt;main+0x4a&gt;\n &#123;\n     a &#x3D; 1;\n41: c7 45 f8 01 00 00 00      mov DWORD PTR [rbp-0x8],0x1 \n48: eb 07                     jmp 51 &lt;main+0x51&gt;\n    &#125; \n    else \n    &#123;\n         a &#x3D; 2;\n4a: c7 45 f8 02 00 00 00   mov DWORD PTR [rbp-0x8],0x2\n51: b8 00 00 00 00         mov eax,0x0\n&#125;\n\n可以看到，这里对于r &#x3D;&#x3D; 0的条件判断，被编译成了cmp和jne这两条指令。\n对于：\ncmp DWORD PTR [rbp-0x4],0x0\ncmp指令比较了前后两个操作数的值，这里的DWORD PTR代表操作的数据类型是32位的整数，而[rbp-0x4]则是一个寄存器的地址。所以，第一个操作数就是从寄存器里拿到的变量r的值。第二个操作数0x0就是我们设定的常量0的16进制表示。cmp指令的比较结果，会存入到条件码寄存器当中去。\n在这里，如果比较的结果是False，也就是0，就把零标志条件码（对应的条件码是ZF，Zero Flag）设置为1。\ncmp指令执行完成之后，PC寄存器会自动自增，开始执行下一条jne的指令。\n对于：\njne 4a &lt;main+0x4a&gt;\njne指令，是jump if not equal的意思，它会查看对应的零标志位。如果为0，会跳转到后面跟着的操作数4a的位置。这个4a，对应这里汇编代码的行号，也就是上面设置的else条件里的第一条指令。\n当跳转发生的时候，PC寄存器就不再是自增变成下一条指令的地址，而是被直接设置成这里的4a这个地址。这个时候，CPU再把4a地址里的指令加载到指令寄存器中来执行。\n4a: c7 45 f8 02 00 00 00     mov DWORD PTR [rbp-0x8],0x2 \n51: b8 00 00 00 00           mov eax,0x0\n\n4a的指令，实际是一条mov指令，第一个操作数和前面的cmp指令一样，是另一个32位整型的寄存器地址，以及对应的2的16进制值0x2。mov指令把2设置到对应的寄存器里去，相当于一个赋值操作。然后，PC寄存器里的值继续自增，执行下一条mov指令。\n下一条指令也是mov，第一个操作数eax，代表累加寄存器，第二个操作数0x0则是16进制的0的表示。这条指令其实没有实际的作用，它的作用是一个占位符。\n函数调用我们先来看个例子：\n&#x2F;&#x2F; function_example.c \n#include &lt;stdio.h&gt; \nint static add(int a, int b) &#123;\n return a+b; \n&#125; \nint main() \n&#123; \nint x &#x3D; 5; \nint y &#x3D; 10; \nint u &#x3D; add(x, y);\n&#125;\n我们把这个程序编译之后：\nint static add(int a, int b) \n&#123;\n 0: 55 push rbp \n1: 48 89 e5 mov rbp,rsp \n4: 89 7d fc mov DWORD PTR [rbp-0x4],edi \n7: 89 75 f8 mov DWORD PTR [rbp-0x8],esi \nreturn a+b; \na: 8b 55 fc mov edx,DWORD PTR [rbp-0x4] \nd: 8b 45 f8 mov eax,DWORD PTR [rbp-0x8] \n10:  5d pop rbp \n13: c3 ret 0000000000000014 &lt;main&gt;:\n int main() \n&#123; \n14: 55 push rbp \n15: 48 89 e5 mov rbp,rsp \n18: 48 83 ec 10 sub rsp,0x10 \n    int x &#x3D; 5; \n1c: c7 45 fc 05 00 00 00 mov DWORD PTR [rbp-0x4],0x5 \n    int y &#x3D; 10; \n23: c7 45 f8 0a 00 00 00 mov DWORD PTR [rbp-0x8],0xa \n    int u &#x3D; add(x, y); \n2a: 8b 55 f8 mov edx,DWORD PTR [rbp-0x8] \n2d: 8b 45 fc mov eax,DWORD PTR [rbp-0x4] 30: 89 d6 mov esi,edx \n32: 89 c7 mov edi,eax 34: e8 c7 ff ff ff call 0 &lt;add&gt; \n39: 89 45 f4 mov DWORD PTR [rbp-0xc],eax \n3c: b8 00 00 00 00 mov eax,0x0 \n&#125; \n41: c9 leave \n42: c3 ret01 d0 add eax,edx \n&#125; \n12:\n在add函数编译之后，代码先执行了一条push指令和一条mov指令；在函数执行结束的时候，又执行了一条pop和一条ret指令。\nadd函数的第0行，push rbp这个指令，就是在进行压栈。这里的rbp又叫栈帧指针（Frame Pointer），是一个存放了当前栈帧位置的寄存器。push rbp就把之前调用函数的返回地址，压到栈顶。\n接着，第1行的一条命令mov rbp, rsp里，则是把rsp这个栈指针（Stack Pointer）的值复制到rbp里，而rsp始终会指向栈顶。这个命令意味着，rbp这个栈帧指针指向的返回地址，变成当前最新的栈顶，也就是add函数的返回地址了。\n而在函数add执行完成之后，又会分别调用第12行的pop rbp来将当前的栈顶出栈，然后调用第13行的ret指令，将程序的控制权返回到出栈后的栈顶，也就是main函数的返回地址。\n拆解程序执行实际上，“C语言代码-汇编代码-机器码” 这个过程，在我们的计算机上进行的时候是由两部分组成的。\n第一个部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。在这三个阶段完成之后，我们就生成了一个可执行文件。\n第二部分，我们通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU从内存中读取指令和数据，来开始真正执行程序。\n\n静态链接程序的链接，是把对应的不同文件内的代码段，合并到一起，成为最后的可执行文件。\n在可执行文件里，我们可以看到，对应的函数名称，像add、main等等，乃至你自己定义的全局可以访问的变量名称对应的地址，存储在一个叫作符号表（Symbols Table）的位置里。符号表相当于一个地址簿，把名字和地址关联了起来。\n经过程序的链接之后，main函数里调用add的跳转地址，不再是下一条指令的地址了，而是add函数的入口地址了。\n链接器会扫描所有输入的目标文件，然后把所有符号表里的信息收集起来，构成一个全局的符号表。然后再根据重定位表，把所有不确定要跳转地址的代码，根据符号表里面存储的地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。\n这个合并代码段的方法，是叫静态链接。\n动态链接在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的共享库（Shared Libraries）。\n要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“地址无关”的。换句话说就是，这段代码，无论加载在哪个内存地址，都能够正常执行。\n\n动态代码库内部的变量和函数调用都是使用相对地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。\n装载程序在运行这些可执行文件的时候，我们其实是通过一个装载器，解析ELF或者PE格式的可执行文件。装载器会把对应的指令和数据加载到内存里面来，让CPU去执行。\n装载器需要满足两个要求：\n1、可执行程序加载后占用的内存空间应该是连续的。因为CPU在执行指令的时候，程序计数器是顺序地一条一条指令执行下去。\n2、我们需要同时加载很多个程序，并且不能让程序自己规定在内存中加载的位置。因为我们现在的计算机通常会同时运行很多个程序，可能你想要的内存地址已经被其他加载了的程序占用了。\n基于上面，我们需要在内存空间地址和整个程序指令指定的内存地址做一个映射。\n把指令里用到的内存地址叫作虚拟内存地址（Virtual Memory Address），实际在内存硬件里面的空间地址，我们叫物理内存地址（Physical Memory Address）。\n内存分页分页是把整个物理内存空间切成一段段固定尺寸的大小。而对应的程序所需要占用的虚拟内存空间，也会同样切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。\n从虚拟内存到物理内存的映射，不再是拿整段连续的内存的物理地址，而是按照一个一个页来的。\n\n分页之后避免了整个程序和硬盘进行交换而产生性能瓶颈。即使内存空间不够，需要让现有的、正在运行的其他程序，通过内存交换释放出一些内存的页出来，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，让整个机器被内存交换的过程给卡住。\n浮点数和定点数我们先来看一个问题，在Chrome浏览器里面通过开发者工具，打开浏览器里的Console，在里面输入“0.3 + 0.6”：\n&gt;&gt;&gt; 0.3 + 0.6\n0.8999999999999999\n下面我们来一步步解释，为什么会这样。\n定点数如果我们用32个比特表示整数，用4个比特来表示0～9的整数，那么32个比特就可以表示8个这样的整数。\n然后我们把最右边的2个0～9的整数，当成小数部分；把左边6个0～9的整数，当成整数部分。这样，我们就可以用32个比特，来表示从0到999999.99这样1亿个实数了。\n这种用二进制来表示十进制的编码方式，叫作BCD编码。这种小数点固定在某一位的方式，我们也就把它称为定点数。\n缺点：\n第一，这样的表示方式有点“浪费”。本来32个比特我们可以表示40亿个不同的数，但是在BCD编码下，只能表示1亿个数。\n第二，这样的表示方式没办法同时表示很大的数字和很小的数字。\n浮点数我们在表示一个很大的数的时候，通常可以用科学计数法来表示。\n在计算机里，我也可以用科学计数法来表示实数。浮点数的科学计数法的表示，有一个IEEE的标准，它定义了两个基本的格式。一个是用32比特表示单精度的浮点数，也就是我们常常说的float或者float32类型。另外一个是用64比特表示双精度的浮点数，也就是我们平时说的double或者float64类型。\n单精度单精度的32个比特可以分成三部分。\n\n第一部分是一个符号位，用来表示是正数还是负数。我们一般用s来表示。在浮点数里，我们不像正数分符号数还是无符号数，所有的浮点数都是有符号的。\n接下来是一个8个比特组成的指数位。我们一般用e来表示。8个比特能够表示的整数空间，就是0～255。我们在这里用1～254映射到-126～127这254个有正有负的数上。\n最后，是一个23个比特组成的有效数位。我们用f来表示。综合科学计数法，我们的浮点数就可以表示成下面这样：$(-1)^s×1.f×2^e$\n特殊值的表示\n以0.5为例子。0.5的符号为s应该是0，f应该是0，而e应该是-1，也就是***$0.5&#x3D; (-1)^0×1.0×2^{-1}&#x3D;0.5$***，对应的浮点数表示，就是32个比特。\n不考虑符号的话，浮点数能够表示的最小的数和最大的数，差不多是***$1.17×10^{-38}$和$3.40×10^{38}$***。\n回到我们最开头，为什么我们用0.3 + 0.6不能得到0.9呢？这是因为，浮点数没有办法精确表示0.3、0.6和0.9。\n浮点数的二进制转化我们输入一个任意的十进制浮点数，背后都会对应一个二进制表示。\n比如：9.1，那么，首先，我们把这个数的整数部分，变成一个二进制。这里的9，换算之后就是1001。\n接着，我们把对应的小数部分也换算成二进制。和整数的二进制表示采用“除以2，然后看余数”的方式相比，小数部分转换成二进制是用一个相似的反方向操作，就是乘以2，然后看看是否超过1。如果超过1，我们就记下1，并把结果减去1，进一步循环操作。在这里，我们就会看到，0.1其实变成了一个无限循环的二进制小数，0.000110011。这里的“0011”会无限循环下去。\n\n结果就是：***$1.0010$$0011$$0011… × 2^3$***\n这里的符号位s &#x3D; 0，对应的有效位f&#x3D;001000110011…。因为f最长只有23位，那这里“0011”无限循环，最多到23位就截止了。于是，f&#x3D;00100011001100110011 001。最后的一个“0011”循环中的最后一个“1”会被截断掉。\n对应的指数为e，代表的应该是3。因为指数位有正又有负，所以指数位在127之前代表负数，之后代表正数，那3其实对应的是加上127的偏移量130，转化成二进制，就是130，对应的就是指数位的二进制，表示出来就是10000010。\n\n最终得到的二进制表示就变成了：\n010000010 0010 0011001100110011 001\n如果我们再把这个浮点数表示换算成十进制， 实际准确的值是9.09999942779541015625。\n浮点数的加法和精度损失浮点数的加法是：先对齐、再计算。\n那我们在计算0.5+0.125的浮点数运算的时候，首先要把两个的指数位对齐，也就是把指数位都统一成两个其中较大的-1。对应的有效位1.00…也要对应右移两位，因为f前面有一个默认的1，所以就会变成0.01。然后我们计算两者相加的有效位1.f，就变成了有效位1.01，而指数位是-1，这样就得到了我们想要的加法后的结果。\n\n其中指数位较小的数，需要在有效位进行右移，在右移的过程中，最右侧的有效位就被丢弃掉了。这会导致对应的指数位较小的数，在加法发生之前，就丢失精度。\n指令周期（Instruction Cycle）计算机每执行一条指令的过程，可以分解成这样几个步骤。\n\nFetch（取得指令），也就是从PC寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把PC寄存器自增，好在未来执行下一条指令。\nDecode（指令译码），也就是根据指令寄存器里面的指令，解析成要进行什么样的操作，是R、I、J中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址。\nExecute（执行指令），也就是实际运行对应的R、I、J这些特定的指令，进行算术逻辑操作、数据传输或者直接的地址跳转。\nFetch - Decode - Execute循环称之为指令周期（Instruction Cycle）。\n\n\n在取指令的阶段，我们的指令是放在存储器里的，实际上，通过PC寄存器和指令寄存器取出指令的过程，是由控制器（Control Unit）操作的。指令的解码过程，也是由控制器进行的。一旦到了执行指令阶段，无论是进行算术操作、逻辑操作的R型指令，还是进行数据传输、条件分支的I型指令，都是由算术逻辑单元（ALU）操作的，也就是由运算器处理的。不过，如果是一个简单的无条件地址跳转，那么我们可以直接在控制器里面完成，不需要用到运算器。\n\n时序逻辑电路有一些电路，只需要给定输入，就能得到固定的输出。这样的电路，我们称之为组合逻辑电路（Combinational Logic Circuit）。\n时序逻辑电路有以下几个特点：\n1、自动运行，时序电路接通之后可以不停地开启和关闭开关，进入一个自动运行的状态。\n2、存储。通过时序电路实现的触发器，能把计算结果存储在特定的电路里面，而不是像组合逻辑电路那样，一旦输入有任何改变，对应的输出也会改变。\n3、时序电路使得不同的事件按照时间顺序发生。\n最常见的就是D触发器，电路的输出信号不单单取决于当前的输入信号，还要取决于输出信号之前的状态。\nPC寄存器PC寄存器就是程序计数器。\n\n加法器的两个输入，一个始终设置成1，另外一个来自于一个D型触发器A。我们把加法器的输出结果，写到这个D型触发器A里面。于是，D型触发器里面的数据就会在固定的时钟信号为1的时候更新一次。\n这样，我们就有了一个每过一个时钟周期，就能固定自增1的自动计数器了。\n最简单的CPU流程\n1、首先，有一个自动计数器会随着时钟主频不断地自增，来作为我们的PC寄存器。\n2、在这个自动计数器的后面，我们连上一个译码器（用来寻址，将指令内存地址转换成指令）。译码器还要同时连着我们通过大量的D触发器组成的内存。\n3、自动计数器会随着时钟主频不断自增，从译码器当中，找到对应的计数器所表示的内存地址，然后读取出里面的CPU指令。\n4、读取出来的CPU指令会通过我们的CPU时钟的控制，写入到一个由D触发器组成的寄存器，也就是指令寄存器当中。\n5、在指令寄存器后面，我们可以再跟一个译码器。这个译码器不再是用来寻址的了，而是把我们拿到的指令，解析成opcode和对应的操作数。\n6、当我们拿到对应的opcode和操作数，对应的输出线路就要连接ALU，开始进行各种算术和逻辑运算。对应的计算结果，则会再写回到D触发器组成的寄存器或者内存当中。\n指令流水线指令流水线指的是把一个指令拆分成一个一个小步骤，从而来减少单条指令执行的“延时”。通过同时在执行多条指令的不同阶段，我们提升了CPU的“吞吐率”。\n如果我们把一个指令拆分成“取指令-指令译码-执行指令”这样三个部分，那这就是一个三级的流水线。如果我们进一步把“执行指令”拆分成“ALU计算（指令执行）-内存访问-数据写回”，那么它就会变成一个五级的流水线。\n五级的流水线，就表示我们在同一个时钟周期里面，同时运行五条指令的不同阶段。\n我们可以看这样一个例子。我们顺序执行这样三条指令。\n1、一条整数的加法，需要200ps。\n2、一条整数的乘法，需要300ps。\n3、一条浮点数的乘法，需要600ps\n如果我们是在单指令周期的CPU上运行，最复杂的指令是一条浮点数乘法，那就需要600ps。那这三条指令，都需要600ps。三条指令的执行时间，就需要1800ps。\n如果我们采用的是6级流水线CPU，每一个Pipeline的Stage都只需要100ps。那么，在这三个指令的执行过程中，在指令1的第一个100ps的Stage结束之后，第二条指令就开始执行了。在第二条指令的第一个100ps的Stage结束之后，第三条指令就开始执行了。这种情况下，这三条指令顺序执行所需要的总时间，就是800ps。那么在1800ps内，使用流水线的CPU比单指令周期的CPU就可以多执行一倍以上的指令数。\n\n流水线设计CPU的风险结构冒险\n可以看到，在第1条指令执行到访存（MEM）阶段的时候，流水线里的第4条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。但是内存在一个时钟周期是没办法都做的。\n解决办法：在高速缓存层面拆分成指令缓存和数据缓存\n在CPU内部的高速缓存部分进行了区分，把高速缓存分成了指令缓存（Instruction Cache）和数据缓存（Data Cache）两部分。\n\n数据冒险1、先写后读\nint main() \n&#123; int a &#x3D; 1;\n  int b &#x3D; 2;\n  a &#x3D; a + 2;\n  b &#x3D; a + 3; &#125;\n\n这里需要保证a和b的值先赋，然后才能进行准确的运算。这个先写后读的依赖关系，我们一般被称之为数据依赖，也就是Data Dependency。\n2、先读后写\nmain()&#123; int a &#x3D; 1;\n  int b &#x3D; 2;\n  a &#x3D; b + a; \n  b &#x3D; a + b; &#125;\n\n这里我们先要读出a &#x3D; b+a，然后才能正确的写入b的值。这个先读后写的依赖，一般被叫作反依赖，也就是Anti-Dependency。\n3、写后再写\nmain()&#123; int a &#x3D; 1;\n  a &#x3D; 2; &#125;\n很明显，两个写入操作不能乱，要不然最终结果就是错误的。这个写后再写的依赖，一般被叫作输出依赖，也就是Output Dependency。\n解决办法：流水线停顿（Pipeline Stall）\n\n如果我们发现了后面执行的指令，会对前面执行的指令有数据层面的依赖关系，那最简单的办法就是“再等等”。我们在进行指令译码的时候，会拿到对应指令所需要访问的寄存器和内存地址。\n在实践过程中，在执行后面的操作步骤前面，插入一个NOP操作，也就是执行一个其实什么都不干的操作。\n在执行的代码中，一旦遇到 if…else 这样的条件分支，或者 for&#x2F;while 循环的时候会发生类似cmp比较指令、jmp和jle这样的条件跳转指令。\n在jmp指令发生的时候，CPU可能会跳转去执行其他指令。jmp后的那一条指令是否应该顺序加载执行，在流水线里面进行取指令的时候，我们没法知道。要等jmp指令执行完成，去更新了PC寄存器之后，我们才能知道，是否执行下一条指令，还是跳转到另外一个内存地址，去取别的指令。\n解决办法： \n缩短分支延迟\n条件跳转指令其实进行了两种电路操作。\n第一种，是进行条件比较。\n第二种，是进行实际的跳转，也就是把要跳转的地址信息写入到PC寄存器。无论是opcode，还是对应的条件码寄存器，还是我们跳转的地址，都是在指令译码（ID）的阶段就能获得的。而对应的条件码比较的电路，只要是简单的逻辑门电路就可以了，并不需要一个完整而复杂的ALU。\n所以，我们可以将条件判断、地址跳转，都提前到指令译码阶段进行，而不需要放在指令执行阶段。对应的，我们也要在CPU里面设计对应的旁路，在指令译码阶段，就提供对应的判断比较的电路。\n分支预测\n最简单的分支预测技术，叫作“假装分支不发生”。顾名思义，自然就是仍然按照顺序，把指令往下执行。\n如果分支预测失败了呢？那我们就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作Zap或者Flush。CPU不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，我们还需要做对应的清除操作。\n\n动态分支预测\n就是记录当前分支的比较情况，直接用当前分支的比较情况，来预测下一次分支时候的比较情况。\n例子：\nclass BranchPrediction &#123;    public static void main(String args[]) &#123;        \n        long start &#x3D; System.currentTimeMillis();\n        for (int i &#x3D; 0; i &lt; 100; i++) &#123;\n            for (int j &#x3D; 0; j &lt;1000; j ++) &#123;\n                for (int k &#x3D; 0; k &lt; 10000; k++) &#123;\n                &#125;\n            &#125;\n        &#125;\n        long end &#x3D; System.currentTimeMillis();\n        System.out.println(&quot;Time spent is &quot; + (end - start));\n                \n        start &#x3D; System.currentTimeMillis();\n        for (int i &#x3D; 0; i &lt; 10000; i++) &#123;\n            for (int j &#x3D; 0; j &lt;1000; j ++) &#123;\n                for (int k &#x3D; 0; k &lt; 100; k++) &#123;\n                &#125;\n            &#125;\n        &#125;\n        end &#x3D; System.currentTimeMillis();\n        System.out.println(&quot;Time spent is &quot; + (end - start) + &quot;ms&quot;);\n    &#125;\n&#125;\n\n输出：\nspent in first loop is 5msTime spent in second loop is 15ms\n\n\n分支预测策略最简单的一个方式，自然是“假定分支不发生”。对应到上面的循环代码，就是循环始终会进行下去。在这样的情况下，上面的第一段循环，也就是内层 k 循环10000次的代码。每隔10000次，才会发生一次预测上的错误。而这样的错误，在第二层 j 的循环发生的次数，是1000次。\n最外层的 i 的循环是100次。每个外层循环一次里面，都会发生1000次最内层 k 的循环的预测错误，所以一共会发生 100 × 1000 &#x3D; 10万次预测错误。\n操作数前推通过流水线停顿可以解决资源竞争产生的问题，但是，插入过多的NOP操作，意味着我们的CPU总是在空转，干吃饭不干活。所以我们提出了操作数前推这样的解决方案。\n1、  add $t0, $s2,$s1\n2、  add $s2, $s1,$t0\n第一条指令，把 s1 和 s2 寄存器里面的数据相加，存入到 t0 这个寄存器里面。\n第二条指令，把 s1 和 t0 寄存器里面的数据相加，存入到 s2 这个寄存器里面。\n\n我们要在第二条指令的译码阶段之后，插入对应的NOP指令，直到前一天指令的数据写回完成之后，才能继续执行。但是这样浪费了两个时钟周期。\n这个时候完全可以在第一条指令的执行阶段完成之后，直接将结果数据传输给到下一条指令的ALU。然后，下一条指令不需要再插入两个NOP阶段，就可以继续正常走到执行阶段。\n\n这样的解决方案，我们就叫作操作数前推（Operand Forwarding），或者操作数旁路（Operand Bypassing）。\nCPU指令乱序执行\n1、在取指令和指令译码的时候，乱序执行的CPU和其他使用流水线架构的CPU是一样的。它会一级一级顺序地进行取指令和指令译码的工作。\n2、在指令译码完成之后，CPU不会直接进行指令执行，而是进行一次指令分发，把指令发到一个叫作保留站（Reservation Stations）的地方。\n3、这些指令不会立刻执行，而要等待它们所依赖的数据，传递给它们之后才会执行。\n4、一旦指令依赖的数据来齐了，指令就可以交到后面的功能单元（Function Unit，FU），其实就是ALU，去执行了。我们有很多功能单元可以并行运行，但是不同的功能单元能够支持执行的指令并不相同。\n5、指令执行的阶段完成之后，我们并不能立刻把结果写回到寄存器里面去，而是把结果再存放到一个叫作重排序缓冲区（Re-Order Buffer，ROB）的地方。\n6、在重排序缓冲区里，我们的CPU会按照取指令的顺序，对指令的计算结果重新排序。只有排在前面的指令都已经完成了，才会提交指令，完成整个指令的运算结果。\n7、实际的指令的计算结果数据，并不是直接写到内存或者高速缓存里，而是先写入存储缓冲区（Store Buffer面，最终才会写入到高速缓存和内存里。\n8、在乱序执行的情况下，只有CPU内部指令的执行层面，可能是“乱序”的。\n例子：\na &#x3D; b + c\nd &#x3D; a * e\nx &#x3D; y * z\n\n里面的 d 依赖于 a 的计算结果，不会在 a 的计算完成之前执行。但是我们的CPU并不会闲着，因为 x &#x3D; y * z 的指令同样会被分发到保留站里。因为 x 所依赖的 y 和 z 的数据是准备好的， 这里的乘法运算不会等待计算 d，而会先去计算 x 的值。\n如果我们只有一个FU能够计算乘法，那么这个FU并不会因为 d 要等待 a 的计算结果，而被闲置，而是会先被拿去计算 x。\n在 x 计算完成之后，d 也等来了 a 的计算结果。这个时候，我们的FU就会去计算出 d 的结果。然后在重排序缓冲区里，把对应的计算结果的提交顺序，仍然设置成 a -&gt; d -&gt; x，而计算完成的顺序是 x -&gt; a -&gt; d。\n在这整个过程中，整个计算乘法的FU都没有闲置，这也意味着我们的CPU的吞吐率最大化了。\n乱序执行，极大地提高了CPU的运行效率。核心原因是，现代CPU的运行速度比访问主内存的速度要快很多。如果完全采用顺序执行的方式，很多时间都会浪费在前面指令等待获取内存数据的时间里。CPU不得不加入NOP操作进行空转。\n超线程\n超线程的CPU，其实是把一个物理层面CPU核心，“伪装”成两个逻辑层面的CPU核心。这个CPU，会在硬件层面增加很多电路，使得我们可以在一个CPU核心内部，维护两个不同线程的指令的状态信息。\n比如，在一个物理CPU核心内部，会有双份的PC寄存器、指令寄存器乃至条件码寄存器。这样，这个CPU核心就可以维护两条并行的指令的状态。\n超线程并不是真的去同时运行两个指令，超线程的目的，是在一个线程A的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU的译码器和ALU就空出来了，那么另外一个线程B，就可以拿来干自己需要的事情。这个线程B可没有对于线程A里面指令的关联和依赖。\n所以超线程只在特定的应用场景下效果比较好。一般是在那些各个线程“等待”时间比较长的应用场景下。比如，我们需要应对很多请求的数据库应用，就很适合使用超线程。各个指令都要等待访问内存数据，但是并不需要做太多计算。\nSIMD加速矩阵乘法SIMD，中文叫作单指令多数据流（Single Instruction Multiple Data）。\n下面是两段示例程序，一段呢，是通过循环的方式，给一个list里面的每一个数加1。另一段呢，是实现相同的功能，但是直接调用NumPy这个库的add方法。\n$ python\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import timeit\n&gt;&gt;&gt; a &#x3D; list(range(1000))\n&gt;&gt;&gt; b &#x3D; np.array(range(1000))\n&gt;&gt;&gt; timeit.timeit(&quot;[i + 1 for i in a]&quot;, setup&#x3D;&quot;from __main__ import a&quot;, number&#x3D;1000000)\n32.82800309999993\n&gt;&gt;&gt; timeit.timeit(&quot;np.add(1, b)&quot;, setup&#x3D;&quot;from __main__ import np, b&quot;, number&#x3D;1000000)\n0.9787889999997788\n&gt;&gt;&gt;\n两个功能相同的代码性能有着巨大的差异，足足差出了30多倍。原因就是，NumPy直接用到了SIMD指令，能够并行进行向量的操作。\n使用循环来一步一步计算的算法呢，一般被称为SISD，也就是单指令单数据（Single Instruction Single Data）的处理方式。如果你手头的是一个多核CPU呢，那么它同时处理多个指令的方式可以叫作MIMD，也就是多指令多数据（Multiple Instruction Multiple Dataa）。\nIntel在引入SSE指令集的时候，在CPU里面添上了8个 128 Bits的寄存器。128 Bits也就是 16 Bytes ，也就是说，一个寄存器一次性可以加载 4 个整数。比起循环分别读取4次对应的数据，时间就省下来了。\n在数据读取到了之后，在指令的执行层面，SIMD也是可以并行进行的。4个整数各自加1，互相之前完全没有依赖，也就没有冒险问题需要处理。只要CPU里有足够多的功能单元，能够同时进行这些计算，这个加法就是4路同时并行的，自然也省下了时间。\n所以，对于那些在计算层面存在大量“数据并行”（Data Parallelism）的计算中，使用SIMD是一个很划算的办法。\n异常和中断异常关于异常，它其实是一个硬件和软件组合到一起的处理过程。异常的前半生，也就是异常的发生和捕捉，是在硬件层面完成的。但是异常的后半生，也就是说，异常的处理，其实是由软件来完成的。\n计算机会为每一种可能会发生的异常，分配一个异常代码（Exception Number）。有些教科书会把异常代码叫作中断向量（Interrupt Vector）。\n异常发生的时候，通常是CPU检测到了一个特殊的信号。这些信号呢，在组成原理里面，我们一般叫作发生了一个事件（Event）。CPU在检测到事件的时候，其实也就拿到了对应的异常代码。\n这些异常代码里，I&#x2F;O发出的信号的异常代码，是由操作系统来分配的，也就是由软件来设定的。而像加法溢出这样的异常代码，则是由CPU预先分配好的，也就是由硬件来分配的。\n拿到异常代码之后，CPU就会触发异常处理的流程。计算机在内存里，会保留一个异常表（Exception Table）。我们的CPU在拿到了异常码之后，会先把当前的程序执行的现场，保存到程序栈里面，然后根据异常码查询，找到对应的异常处理程序，最后把后续指令执行的指挥权，交给这个异常处理程序。\n\n异常的分类中断、陷阱、故障和中止\n第一种异常叫中断（Interrupt）。顾名思义，自然就是程序在执行到一半的时候，被打断了。\n第二种异常叫陷阱（Trap）。陷阱，其实是我们程序员“故意“主动触发的异常。就好像你在程序里面打了一个断点，这个断点就是设下的一个”陷阱”。\n第三种异常叫故障（Fault）。比如，我们在程序执行的过程中，进行加法计算发生了溢出，其实就是故障类型的异常。\n最后一种异常叫中止（Abort）。与其说这是一种异常类型，不如说这是故障的一种特殊情况。当CPU遇到了故障，但是恢复不过来的时候，程序就不得不中止了。\n异常的处理：上下文切换在实际的异常处理程序执行之前，CPU需要去做一次“保存现场”的操作。有了这个操作，我们才能在异常处理完成之后，重新回到之前执行的指令序列里面来。\n因为异常情况往往发生在程序正常执行的预期之外，比如中断、故障发生的时候。所以，除了本来程序压栈要做的事情之外，我们还需要把CPU内当前运行程序用到的所有寄存器，都放到栈里面。最典型的就是条件码寄存器里面的内容。\n像陷阱这样的异常，涉及程序指令在用户态和内核态之间的切换。对应压栈的时候，对应的数据是压到内核栈里，而不是程序栈里。\n虚拟机技术解释型虚拟机我们把原先的操作系统叫作宿主机（Host），把能够有能力去模拟指令执行的软件，叫作模拟器（Emulator），而实际运行在模拟器上被“虚拟”出来的系统呢，我们叫客户机（Guest VM）。\n例如在windows上跑的Android模拟器，或者能在Windows下运行的游戏机模拟器。\n这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。比如，Android手机用的CPU是ARM的，而我们的开发机用的是Intel X86的，两边的CPU指令集都不一样，但是一样可以正常运行。\n缺陷：\n第一个是，我们做不到精确的“模拟”。很多的老旧的硬件的程序运行，要依赖特定的电路乃至电路特有的时钟频率，想要通过软件达到100%模拟是很难做到的。\n第二个是这种解释执行的方式，性能实在太差了。因为我们并不是直接把指令交给CPU去执行的，而是要经过各种解释和翻译工作。\nType-1和Type-2虚拟机如果我们需要一个“全虚拟化”的技术，可以在现有的物理服务器的硬件和操作系统上，去跑一个完整的、不需要做任何修改的客户机操作系统（Guest OS），有一个很常用的一个解决方案，就是加入一个中间层。在虚拟机技术里面，这个中间层就叫作虚拟机监视器，英文叫VMM（Virtual Machine Manager）或者Hypervisor。\nType-2虚拟机在Type-2虚拟机里，我们上面说的虚拟机监视器好像一个运行在操作系统上的软件。你的客户机的操作系统呢，把最终到硬件的所有指令，都发送给虚拟机监视器。而虚拟机监视器，又会把这些指令再交给宿主机的操作系统去执行。\nType-1虚拟机在数据中心里面用的虚拟机，我们通常叫作Type-1型的虚拟机。客户机的指令交给虚拟机监视器之后呢，不再需要通过宿主机的操作系统，才能调用硬件，而是可以直接由虚拟机监视器去调用硬件。\n在Type-1型的虚拟机里，我们的虚拟机监视器其实并不是一个操作系统之上的应用层程序，而是一个嵌入在操作系统内核里面的一部分。\n\nDocker在我们实际的物理机上，我们可能同时运行了多个的虚拟机，而这每一个虚拟机，都运行了一个属于自己的单独的操作系统。多运行一个操作系统，意味着我们要多消耗一些资源在CPU、内存乃至磁盘空间上。\n在服务器领域，我们开发的程序都是跑在Linux上的。其实我们并不需要一个独立的操作系统，只要一个能够进行资源和环境隔离的“独立空间”就好了。\n\n通过Docker，我们不再需要在操作系统上再跑一个操作系统，而只需要通过容器编排工具，比如Kubernetes或者Docker Swarm，能够进行各个应用之间的环境和资源隔离就好了。\n存储器SRAMSRAM（Static Random-Access Memory，静态随机存取存储器），被用在CPU Cache中。\nSRAM之所以被称为“静态”存储器，是因为只要处在通电状态，里面的数据就可以保持存在。而一旦断电，里面的数据就会丢失了。在SRAM里面，一个比特的数据，需要6～8个晶体管。所以SRAM的存储密度不高。同样的物理空间下，能够存储的数据有限。不过，因为SRAM的电路简单，所以访问速度非常快。\n在CPU里，通常会有L1、L2、L3这样三层高速缓存。每个CPU核心都有一块属于自己的L1高速缓存。\nL2的Cache同样是每个CPU核心都有的，不过它往往不在CPU核心的内部。所以，L2 Cache的访问速度会比L1稍微慢一些。\nL3 Cache，则通常是多个CPU核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。\nDRAM内存用的芯片是一种叫作DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起SRAM来说，它的密度更高，有更大的容量，而且它也比SRAM芯片便宜不少。\nDRAM被称为“动态”存储器，是因为DRAM需要靠不断地“刷新”，才能保持数据被存储起来。DRAM的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。\n\nCPU Cache目前看来，一次内存的访问，大约需要120个CPU Cycle，这也意味着，在今天，CPU和内存的访问速度已经有了120倍的差距。\n为了弥补两者之间的性能差异，我们能真实地把CPU的性能提升用起来，而不是让它在那儿空转，我们在现代CPU中引入了高速缓存。\nCPU从内存中读取数据到CPU Cache的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在CPU Cache里面，我们把它叫作Cache Line（缓存块）。\n在我们日常使用的Intel服务器或者PC里，Cache Line的大小通常是64字节。\n直接映射Cache（Direct Mapped Cache）对于读取内存中的数据，我们首先拿到的是数据所在的内存块（Block）的地址。而直接映射Cache采用的策略，就是确保任何一个内存块的地址，始终映射到一个固定的CPU Cache地址（Cache Line）。而这个映射关系，通常用mod运算（求余运算）来实现。\n比如说，我们的主内存被分成0～31号这样32个块。我们一共有8个缓存块。用户想要访问第21号内存块。如果21号内存块内容在缓存块中的话，它一定在5号缓存块（21 mod 8 &#x3D; 5）中。\n\n在对应的缓存块中，我们会存储一个组标记（Tag）。这个组标记会记录，当前缓存块内存储的数据对应的内存块，而缓存块本身的地址表示访问地址的低N位。\n除了组标记信息之外，缓存块中还有两个数据。一个自然是从主内存中加载来的实际存放的数据，另一个是有效位（valid bit）。啥是有效位呢？它其实就是用来标记，对应的缓存块中的数据是否是有效的，确保不是机器刚刚启动时候的空数据。如果有效位是0，无论其中的组标记和Cache Line里的数据内容是什么，CPU都不会管这些数据，而要直接访问内存，重新加载数据。\nCPU在读取数据的时候，并不是要读取一整个Block，而是读取一个他需要的整数。这样的数据，我们叫作CPU里的一个字（Word）。具体是哪个字，就用这个字在整个Block里面的位置来决定。这个位置，我们叫作偏移量（Offset）。\n一个内存的访问地址，最终包括高位代表的组标记、低位代表的索引，以及在对应的Data Block中定位对应字的位置偏移量。\n\n如果内存中的数据已经在CPU Cache里了，那一个内存地址的访问，就会经历这样4个步骤：\n1、根据内存地址的低位，计算在Cache中的索引\n2、判断有效位，确认Cache中的数据是有效的；\n3、对比内存访问地址的高位，和Cache中的组标记，确认Cache中的数据就是我们要访问的内存数据，从Cache Line中读取到对应的数据块（Data Block）；\n4、根据内存地址的Offset位，从Data Block中，读取希望读取到的字。\nCPU高速缓存的写入每一个CPU核里面，都有独立属于自己的L1、L2的Cache，然后再有多个CPU核共用的L3的Cache、主内存。\n\n写直达（Write-Through）最简单的一种写入策略，叫作写直达（Write-Through）。在这个策略里，每一次数据都要写入到主内存里面。在写直达的策略里面，写入前，我们会先去判断数据是否已经在Cache里面了。如果数据已经在Cache里面了，我们先把数据写入更新到Cache里面，再写入到主内存里面；如果数据不在Cache里，我们就只更新主内存。\n这个策略很慢。无论数据是不是在Cache里面，我们都需要把数据写到主内存里面。\n#####写回（Write-Back）\n\n如果发现我们要写入的数据，就在CPU Cache里面，那么我们就只是更新CPU Cache里面的数据。同时，我们会标记CPU Cache里的这个Block是脏（Dirty）的。所谓脏的，就是指这个时候，我们的CPU Cache里面的这个Block的数据，和主内存是不一致的。\n如果我们发现，我们要写入的数据所对应的Cache Block里，放的是别的内存地址的数据，那么我们就要看一看，那个Cache Block里面的数据有没有被标记成脏的。如果是脏的话，我们要先把这个Cache Block里面的数据，写入到主内存里面。\n然后，再把当前要写入的数据，写入到Cache里，同时把Cache Block标记成脏的。如果Block里面的数据没有被标记成脏的，那么我们直接把数据写入到Cache里面，然后再把Cache Block标记成脏的就好了。\nMESI协议：让多核CPU的高速缓存保持一致#MESI协议，是一种叫作写失效（Write Invalidate）的协议。在写失效协议里，只有一个CPU核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个CPU核心写入Cache之后，它会去广播一个“失效”请求告诉所有其他的CPU核心。其他的CPU核心，只是去判断自己是否也有一个“失效”版本的Cache Block，然后把这个也标记成失效的就好了。\nMESI协议对Cache Line的四个不同的标记，分别是：\n\nM：代表已修改（Modified）\nE：代表独占（Exclusive）\nS：代表共享（Shared）\nI：代表已失效（Invalidated）\n\n所谓的“已修改”，就是我们上一讲所说的“脏”的Cache Block。Cache Block里面的内容我们已经更新过了，但是还没有写回到主内存里面。\n所谓的“已失效“，自然是这个Cache Block里面的数据已经失效了，我们不可以相信这个Cache Block里面的数据。\n在独占状态下，对应的Cache Line只加载到了当前CPU核所拥有的Cache里。其他的CPU核，并没有加载对应的数据到自己的Cache里。这个时候，如果要向独占的Cache Block写入数据，我们可以自由地写入数据，而不需要告知其他CPU核。\n在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个CPU核心，也把对应的Cache Block，从内存里面加载到了自己的Cache里来。\n而在共享状态下，因为同样的数据在多个CPU核心的Cache里都有。所以，当我们想要更新Cache里面的数据的时候，不能直接修改，而是要先向所有的其他CPU核心广播一个请求，要求先把其他CPU核心里面的Cache，都变成无效的状态，然后再更新当前Cache里面的数据。这个广播操作，一般叫作RFO（Request For Ownership），也就是获取当前对应Cache Block数据的所有权。\n内存\n内存是五大组成部分里面的存储器，我们的指令和数据，都需要先加载到内存里面，才会被CPU拿去执行。\n我们的内存需要被分成固定大小的页（Page），然后再通过虚拟内存地址（Virtual Address）到物理内存地址（Physical Address）的地址转换（Address Translation），才能到达实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。\n页表想要把虚拟内存地址，映射到物理内存地址，最直观的办法，就是来建一张映射表。虚拟内存里面的页，到物理内存里面的页的一一映射。这个映射表，在计算机里面，就叫作页表（Page Table）。\n页表这个地址转换的办法，会把一个内存地址分成页号（Directory）和偏移量（Offset）两个部分。\n对于一个内存地址转换，其实就是这样三个步骤：\n1、把虚拟内存地址，切分成页号和偏移量的组合\n2、从页表里面，查询出虚拟页号，对应的物理页号\n3、直接拿物理页号，加上前面的偏移量，就得到了物理内存地址\n\n多级页表（Multi-Level Page Table）大部分进程所占用的内存是有限的，需要的页也自然是很有限的。我们只需要去存那些用到的页之间的映射关系就好了。\n在整个进程的内存地址空间，通常是“两头实、中间空”。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。\n所以，在一个实际的程序进程里面，虚拟内存占用的地址空间，通常是两段连续的空间。\n我们以一个4级的多级页表为例，来看一下。\n\n对应的，一个进程会有一个4级页表。我们先通过4级页表索引，找到4级页表里面对应的条目（Entry）。这个条目里存放的是一张3级页表所在的位置。4级页面里面的每一个条目，都对应着一张3级页表，所以我们可能有多张3级页表。\n找到对应这张3级页表之后，我们用3级索引去找到对应的3级索引的条目。3级索引的条目再会指向一个2级页表。同样的，2级页表里我们可以用2级索引指向一个1级页表。\n而最后一层的1级页表里面的条目，对应的数据内容就是物理页号了。在拿到了物理页号之后，我们同样可以用“页号+偏移量”的方式，来获取最终的物理内存地址。\nTLB加速地址转换程序里面的每一个进程，都有一个属于自己的虚拟内存地址空间。我们可以通过地址转换来获得最终的实际物理地址。我们每一个指令都存放在内存里面，每一条数据都存放在内存里面。“地址转换”是一个非常高频的动作，“地址转换”的性能就变得至关重要了。\n多级页表让原本只需要访问一次内存的操作，变成了需要访问4次内存，才能找到物理页号。程序所需要使用的指令，都顺序存放在虚拟内存里面。我们执行的指令，也是一条条顺序执行下去的。\n于是，计算机工程师们专门在CPU里放了一块缓存芯片。这块缓存芯片我们称之为TLB，全称是地址变换高速缓冲（Translation-Lookaside Buffer）。这块缓存存放了之前已经进行过地址转换的查询结果。这样，当同样的虚拟地址需要进行地址转换的时候，我们可以直接在TLB里面查询结果，而不需要多次访问内存来完成一次转换。\nTLB和我们前面讲的CPU的高速缓存类似，可以分成指令的TLB和数据的TLB，也就是ITLB和DTLB。\n\n为了性能，我们整个内存转换过程也要由硬件来执行。在CPU芯片里面，我们封装了内存管理单元（MMU，Memory Management Unit）芯片，用来完成地址转换。和TLB的访问和交互，都是由这个MMU控制的。\nI&#x2F;O我们先来看一个固态硬盘的Benchmark图：\n\n“4K”的指标就是我们的程序，去随机读取磁盘上某一个4KB大小的数据，一秒之内可以读取到多少数据。\n我们拿这个40MB&#x2F;s和一次读取4KB的数据算一下。 40MB &#x2F; 4KB &#x3D; 10,000 也就是说，一秒之内，这块SSD硬盘可以随机读取1万次的4KB的数据。如果是写入的话呢，会更多一些，90MB &#x2F;4KB 差不多是2万多次。\n这个每秒读写的次数，我们称之为IOPS，也就是每秒输入输出操作的次数。DTR（Data Transfer Rate，数据传输率）\n我们在实际的应用开发当中，对于数据的访问，更多的是随机读写，而不是顺序读写。\n诊断 I&#x2F;O瓶颈首先看一下CPU有没有在等待io操作。\ntop\n\ntop - 06:26:30 up 4 days, 53 min,  1 user,  load average: 0.79, 0.69, 0.65\nTasks: 204 total,   1 running, 203 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 20.0 us,  1.7 sy,  0.0 ni, 77.7 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st\nKiB Mem:   7679792 total,  6646248 used,  1033544 free,   251688 buffers\nKiB Swap:        0 total,        0 used,        0 free.  4115536 cached Mem\nwa的指标，这个指标就代表着iowait，也就是CPU等待IO完成操作花费的时间占CPU的百分比。\n如果iowait很大，那么就可以去看看实际的I&#x2F;O操作情况是什么样的。使用iostat，就能够看到实际的硬盘读写情况。\n$ iostat\n\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n17.02    0.01    2.18    0.04    0.00   80.76\nDevice:            tps    kB_read&#x2F;s    kB_wrtn&#x2F;s    kB_read    kB_wrtn\nsda               1.81         2.02        30.87     706768   10777408\n\ntps指标，其实就对应着我们上面所说的硬盘的IOPS性能。而kB_read&#x2F;s和kB_wrtn&#x2F;s指标，就对应着我们的数据传输率的指标。\n使用iotop找出到底是哪一个进程是这些I&#x2F;O读写的来源。\n$ iotop\n\nTotal DISK READ :       0.00 B&#x2F;s | Total DISK WRITE :      15.75 K&#x2F;s\nActual DISK READ:       0.00 B&#x2F;s | Actual DISK WRITE:      35.44 K&#x2F;s\nTID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND\n104 be&#x2F;3 root        0.00 B&#x2F;s    7.88 K&#x2F;s  0.00 %  0.18 % [jbd2&#x2F;sda1-8]\n383 be&#x2F;4 root        0.00 B&#x2F;s    3.94 K&#x2F;s  0.00 %  0.00 % rsyslogd -n [rs:main Q:Reg]\n1514 be&#x2F;4 www-data    0.00 B&#x2F;s    3.94 K&#x2F;s  0.00 %  0.00 % nginx: worker process\n\n\n硬盘机械硬盘\n一块机械硬盘是由盘面、磁头和悬臂三个部件组成的。\n首先，自然是盘面（Disk Platter）。盘面其实就是我们实际存储数据的盘片。\n我们的硬盘有5400转的、7200转的，乃至10000转的。这个多少多少转，指的就是盘面中间电机控制的转轴的旋转速度，英文单位叫RPM，也就是每分钟的旋转圈数（Rotations Per Minute）。\n磁头：数据并不能直接从盘面传输到总线上，而是通过磁头，从盘面上读取到，然后再通过电路信号传输给控制电路、接口，再到总线上的。通常，我们的一个盘面上会有两个磁头，分别在盘面的正反面。\n悬臂链接在磁头上，并且在一定范围内会去把磁头定位到盘面的某个特定的磁道（Track）上。\n一个盘面通常是圆形的，由很多个同心圆组成，每一个同心圆都是一个磁道。每个磁道都有自己的一个编号。\n磁道，会分成一个一个扇区（Sector）。上下平行的一个一个盘面的相同扇区呢，我们叫作一个柱面（Cylinder）。\n读取数据，其实就是两个步骤。\n把盘面旋转到某一个位置。在这个位置上，我们的悬臂可以定位到整个盘面的某一个子区间。\n把我们的悬臂移动到特定磁道的特定扇区，也就在这个“几何扇区”里面，找到我们实际的扇区。找到之后，我们的磁头会落下，就可以读取到正对着扇区的数据。\n进行一次硬盘上的随机访问，需要的时间由两个部分组成。\n第一个部分，叫作平均延时（Average Latency）。这个时间，其实就是把我们的盘面旋转，把几何扇区对准悬臂位置的时间。这个时间很容易计算，它其实就和我们机械硬盘的转速相关。\n随机情况下，平均找到一个几何扇区，我们需要旋转半圈盘面。上面7200转的硬盘，那么一秒里面，就可以旋转240个半圈。那么，这个平均延时就是：1s &#x2F; 240 &#x3D; 4.17ms\n第二个部分，叫作平均寻道时间（Average Seek Time），也就是在盘面选转之后，我们的悬臂定位到扇区的的时间。我们现在用的HDD硬盘的平均寻道时间一般在4-10ms。\nSSD硬盘现在新的大容量SSD硬盘是由很多个裸片（Die）叠在一起的，就好像我们的机械硬盘把很多个盘面（Platter）叠放再一起一样，这样可以在同样的空间下放下更多的容量。\n\n一张裸片上可以放多个平面（Plane），一般一个平面上的存储容量大概在GB级别。一个平面上面，会划分成很多个块（Block），一般一个块（Block）的存储大小， 通常几百KB到几MB大小。一个块里面，还会区分很多个页（Page），就和我们内存里面的页一样，一个页的大小通常是4KB。\n对于SSD硬盘来说，数据的写入叫作Program。写入不能像机械硬盘一样，通过覆写（Overwrite）来进行的，而是要先去擦除（Erase），然后再写入。\nSSD的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个页（Page）。SSD的擦除单位必须按照块来擦除。\nSSD的使用寿命，其实是每一个块（Block）的擦除的次数。\nSLC的芯片，可以擦除的次数大概在10万次，MLC就在1万次左右，而TLC和QLC就只在几千次了。\nSSD读写的生命周期白色代表这个页从来没有写入过数据，绿色代表里面写入的是有效的数据，红色代表里面的数据，在我们的操作系统看来已经是删除的了。\n\n一开始，所有块的每一个页都是白色的。随着我们开始往里面写数据，里面的有些页就变成了绿色。\n然后，因为我们删除了硬盘上的一些文件，所以有些页变成了红色。但是这些红色的页，并不能再次写入数据。因为SSD硬盘不能单独擦除一个页，必须一次性擦除整个块，所以新的数据，我们只能往后面的白色的页里面写。这些散落在各个绿色空间里面的红色空洞，就好像硬盘碎片。\n如果有哪一个块的数据一次性全部被标红了，那我们就可以把整个块进行擦除。它就又会变成白色，可以重新一页一页往里面写数据。\n在快要没有白色的空页去写入数据的时候，SSD会做一次类似于Windows里面“磁盘碎片整理”或者Java里面的“内存垃圾回收”工作。找一个红色空洞最多的块，把里面的绿色数据，挪到另一个块里面去，然后把整个块擦除，变成白色，可以重新写入数据。\nDMA为什么要发明DMA技术？就目前而言I&#x2F;O速度如何提升，比起CPU，总还是太慢。如果我们对于I&#x2F;O的操作，都是由CPU发出对应的指令，然后等待I&#x2F;O设备完成操作之后返回，那CPU有大量的时间其实都是在等待I&#x2F;O设备完成操作。\n但是，这个CPU的等待，在很多时候，其实并没有太多的实际意义。我们对于I&#x2F;O设备的大量操作，其实都只是把内存里面的数据，传输到I&#x2F;O设备而已。\n因此，计算机工程师们，就发明了DMA技术，也就是直接内存访问（Direct Memory Access）技术，来减少CPU等待的时间。\nDMA有什么用？本质上，DMA技术就是我们在主板上放一块独立的芯片。在进行内存和I&#x2F;O设备的数据传输的时候，我们不再通过CPU来控制数据传输，而直接通过DMA控制器（DMA Controller，简称DMAC）。\n当传输大量数据的时候，DMAC可以等数据到齐了，再发送信号，给到CPU去处理，而不是让CPU在那里忙等待。\nDMAC是怎么控制数据传输的？DMAC其实也是一个特殊的I&#x2F;O设备，它和CPU以及其他I&#x2F;O设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。\n想要主动发起数据传输，必须要是一个主设备才可以，CPU就是主设备。而我们从设备（比如硬盘）只能接受数据传输。\nDMAC它既是一个主设备，又是一个从设备。对于CPU来说，它是一个从设备；对于硬盘这样的IO设备来说呢，它又变成了一个主设备。\n我们下面看一张图：\n\n1、首先，CPU还是作为一个主设备，向DMAC设备发起请求。这个请求，其实就是在DMAC里面修改配置寄存器。\n2、CPU修改DMAC的配置的时候，会告诉DMAC这样几个信息：\n\n首先是源地址的初始值以及传输时候的地址增减方式。\n\n所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。\n\n其次是目标地址初始值和传输时候的地址增减方式。\n\n第三个是要传输的数据长度\n\n\n3、设置完这些信息之后，DMAC就会变成一个空闲的状态（Idle）。\n4、如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向DMAC发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。\n5、然后，我们的DMAC需要再通过一个额外的连线响应这个申请。\n6、于是，DMAC这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了DMAC的控制器里面。\n7、然后，DMAC再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。\n8、DMAC会反复进行上面第6、7步的操作，直到DMAC的寄存器里面设置的数据长度传输完成。\n9、数据传输完成之后，DMAC重新回到第3步的空闲状态。\n结束语以上，是我在大二期间《计算机组成原理》课程的全部笔记，全部由个人独自整理收集。\n","slug":"computer-organization","date":"2022-07-05T07:02:41.000Z","categories_index":"Education,ComputerScience","tags_index":"Education,ComputerScience","author_index":"网工的狗"}]